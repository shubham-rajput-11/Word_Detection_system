{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2a6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851d563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f959840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefd4cb",
   "metadata": {},
   "source": [
    "# Getting a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0881089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training data\n",
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.EMNIST(\n",
    "        root = \"data\",\n",
    "        split = \"letters\",\n",
    "        train = True,\n",
    "        download = True,\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            lambda img : torchvision.transforms.functional.rotate(img,-90),\n",
    "            lambda img : torchvision.transforms.functional.hflip(img),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ]),\n",
    "        target_transform = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0460f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup testing datasets\n",
    "test_data = datasets.EMNIST(\n",
    "        root = \"data\",\n",
    "        split = \"letters\",\n",
    "        train = False,\n",
    "        download = True,\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            lambda img : torchvision.transforms.functional.rotate(img,-90),\n",
    "            lambda img : torchvision.transforms.functional.hflip(img),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ]),\n",
    "        target_transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c8b5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N/A',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f36965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N/A': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx = train_data.class_to_idx\n",
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c705e77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23,  7, 16,  ..., 13, 15, 19])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ea6dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28]) -> [color_channels, height, width]\n",
      "Image label: w\n"
     ]
    }
   ],
   "source": [
    "# # check the shape of our image\n",
    "image,label = train_data[0]\n",
    "print(f\"Image shape: {image.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Image label: {class_names[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8d6a1",
   "metadata": {},
   "source": [
    "# Visualizing images from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b33531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Image shape :torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOdElEQVR4nO3cbayehV3H8f91TtvTFlpK6QjYJx42RksHWJ4S2rsxbpjQuMBUzFgy3XzhO40JuhfGRH1BJhNfmPhmQ7fEBB8gsI0t6FwyYg8UqYAUaDbKJq7lIe2gBUofTs997ssXxn9Gohn/C3qfO/f5fF42/eW6OeeUby9I/k3btm0AQERMzPcHAGB0iAIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIosOA8++yz0TRNPPTQQ/lrTz31VDRNE1u3bn3X77355pvjhhtuGPZHhHkjCiw4W7ZsiVWrVsWuXbvy16anp2NiYiL27t0bb7/9dkREDAaD2L17d+zYsWO+PioMnSiw4ExMTMS2bdtieno6f216ejpuvfXWaJomdu/eHRGRgej1evP1UWHoRIEFqdfrxdNPPx3Hjx+PiIhHH300du7cGVdffXXGYnp6Opqmie3bt8/nR4WhWjTfHwDmQ6/Xi36/H48//nisX78+Dh8+HL1eL/bt2/euKGzevDlWr149z58WhsebAgvStddeG0uXLo1du3bF9PR0nH/++XHZZZdFr9eLPXv2xMzMTExPT/tPRyw43hRYkJYsWRLXX399TE9Px4YNG/Jf/r1eL2ZmZuLee++NQ4cO+Z/MLDjeFFiwer1ePPHEE/HII49kFNasWRObNm2Ku+66K38PLCSiwILV6/Xi5MmTcfDgwXf9y3/Hjh2xf//+uOiii2LdunXz+Alh+ESBBevGG2+MycnJWLFiRVx11VX56z/9n5JgoWnatm3n+0MAMBq8KQCQRAGAJAoAJFEAIIkCAEkUAEjv+czFTRO3ncnPAcAZ9t3B/T/z93hTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAes8H8cZRs6jDP35T72g7e7r+HIB54E0BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBpbA7iLVq3trx56fMby5vB4ra82fCdk+VN89gz5Q3A++VNAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASGNzJfVHv72hvHnkc18qb9ZMLitvvvJrF5U33/r5nytvIiLamZlOu3HT5WrugdvrV3PX/dOR8maw74XyJiIi2vqFXqjypgBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgDQ2B/H6y+vHwpZPTJY3E9GUN9cs/a/y5uELri5vIiL6Pz7YaTfKJleuLG/2/279QOI3fv0vypvbt/1WebP2jz5a3kREDJ7/QafdyOrw5y8iopnstqtqZ08P5TmjxpsCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQDS2BzEW/X9+qG6759eUt5cN1WexJVL5sqbQ59YV39QRJz3tVfro0H983XRLOr24/b6r1xR3vzJLfeVN5cvrn9zH956T3nzC5///fImIuLSOzrNhqLL9/boZ67r9Kw3rqwfv1x0vP7334u/tLe8GRw/Xt6MGm8KACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIY3MQb83Tb5c3+2bWljfXTdUPzk01HY6Fbakf/YqIWLO4/qx2ZkgH8TZ/uNPul37v0fLm02f/pLyZbOp/R7pwcnl584WdD5U3EREP/uH68qadmSlvFm2sP+fFP1td3uzefnd5ExFx7sSy8mYQ9T9PVw5+p7zZeOee8iYiou33O+3OBG8KACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIY3MQb9ws3XCs025i1Tnlzdyhw+VNs6j+o/OTG84tbyIifnnlMx1WTadnDcOVUwc77b55wTXlTXv0rfLm5U/VD+J97Ya/Km+6HLbr6ujgVHmz8qX6Eb120O2Q5SjxpgBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKTxuZI6ObpXMbv4yJrXO+1mz1r+AX+S/1uz+cPlzdm3vdbpWVcumeuwGt0f7S1LZjvtDn1iXXlzdMva8ubOnX9f3lw/Vb8OOtlMljcRETNt/et3+wu3lzerH3i2vBkMuvysjhZvCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKN7NazopVtWlDc7z/phhycN5+DcJ8/f22l3/zkfL28mVtS/difvPlXePLz5vvImImKqqf+YTjbD+ftOl+csiyWdnrXrT/+yvOnyteumfpByrh10etKTM/VDeq/9a/2Y4PrjB8qbceBNAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAafQO4jX1w1oREc1H3ylvzpnodpisaljH2SIiYlH9WUc+taW8+damu8ubqWZZeRMx5K/fEHT951neDOfntYsux+0O9E90etZnv31HebPpbw+WN/3yYjyM1582AN4XUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKN3EK9tO82mHltR3jx/Xf343jVDukk22dQPjEVE9M9eXN4c3l4//XXeRLfjdoy+Lsft+jFX3nz8m/XDdhERl3/5zfKm/+P6QbyFypsCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQmrZ9b2dJb5q47Ux/lvdl0fp15c2b99RPnn7vY/9Y3kw19culbw1OljcREY+fWlXeXDt1pLw5t8OV1EF0u4D7w9mZ8uaCyfpzVk4sLW8mm9H+e1WXi6ddvk93vXFFefPYtg+VNxERg2PHOu2I+O7g/p/5e0b7JxqAoRIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYC0aL4/wAelfevt8ubVly8rb+a2dDjq1tQnZzdT9VFE3LSsyyG9+nG7Lv7jdP04W0TEZ++9o7xZ/rGj5c2ea+8tb6LDwblRP6LX5ft0/1d/sby54Nju8oYzb7R/OgEYKlEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEhjcxAv1l9Yntx+zZ7yZqoZzpdsmEfT5jocdRtE/TDgA29eV95ERFxy/5vlTfvAZHnz7INz5c3VS8bnj9D/eu7U+vLmwkfrByk7nJZkCLwpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgjc01r0GHw2SXLj18Bj7JwvDAO2vKm3/5mxs7PeuCF58pbybPW13ezEVT3oyj2bZ+TDDmnLcbF94UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQxuYgHv9jrh0M5Tl/fN+ny5uLv/xkp2cNZk+XN83F68ub9ZMz5U3E8g6bbrp8b98YnCxv/vyfP1neXLb/ufLGCb3R5E0BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIrqQSr82dKG82fKd+fbPtcO20q/bAq+XN9Km15c2vnnW0vBmmvz66tby55MFT5c3g+PHyhtHkTQGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAMlBvDHTj7ny5jf3f6a8WfL0/vJmUF50156aKW+OzS3r8KThHcQ72dYPCt7zbzvKm8v3/qC8Geb3ljPLmwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJKDeCNqru12YuzIXP0Q3Cu715Y3G08cKG94f54/vbi8Oe+J+h/xwfET5Q3jw5sCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSg3hj5nsnN5Y36x6pH9Gju67HDvfN1A8Xrtl7rLxpB3PlDePDmwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJKDeCPq6OBkp92dz91c3lz84qHypl9ejL4fnTq/Plr56gf/Qf4fXT7fxMnZ8sY5vIXNmwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJDG5krq61tXljdXTL3S4UlNeTHXDsqbB9/5SHkTEbHhi21503+5y9dhtLWzp8ubB769rbz53G88Xt4cmVta3kREfP3r28ubjfuf7PQsFi5vCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASGNzEO/NTfVDcJuW1I+mRUx12NR99aUbO+3OO/xWedPv9KTxc+k/HClvbun/QXlz1sv1n9WIiEse/s/ypt/hMCALmzcFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCksTmIt+hEU96cGMyVN2dPlicx09ZPzvUf/FD9QREx99q/d9oRMbfvhfJm4wv1P0LtoNtBvH6Hn1eo8qYAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYA0NgfxLv3KgfLm47NfKG9mzh2UN4uP1dt78d89U95ERAz69eN7dNf6ejNmvCkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBpbK6k9l9+pbzZ8MVD9Qc1w+noYPb0UJ4D8NO8KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAII3NQbwu2n5/vj8CwEjxpgBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgNS0bdvO94cAYDR4UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg/Tdg8mQ629e/jgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image, label = train_data[0]\n",
    "print(f\" Image shape :{image.shape}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "plt.imshow(image.squeeze());    # matplotlib only need height and width    .......squeeze():= remove singular dimension that is greyscale\n",
    "plt.title(class_names[label]);\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7cd9562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap =\"gray\")\n",
    "plt.title(class_names[label]);\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b7650",
   "metadata": {},
   "source": [
    "# Prepare for DataLoader\n",
    "\n",
    "right now, our data is in the form of pytorch Datasets.<br>\n",
    "Dataloader turns  our <b>dataset into a python iterable</b><br>\n",
    "More specifically , we want to turn our data into <b>batches</b> (or mini-batches).<br><br>\n",
    "Why would we do this ?<br>\n",
    "1. It is more computational efficient , as in, your computing hardware may not be able to look (store in memory) at 60,000 images in one hit. so we break it down to 32 images at a time (batch size of 32)\n",
    "2. It gives our neural network more chances to update its gradients per epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943498b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb69ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#setup batch size hyperparameter \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# turn datasets into iterables \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "                    dataset = train_data,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    shuffle = True)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "                    dataset = test_data,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6409bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader (<torch.utils.data.dataloader.DataLoader object at 0x000000C4518829D0>, <torch.utils.data.dataloader.DataLoader object at 0x000000C45187CF10>)\n",
      "length of train data: 124800\n",
      "length of train_dataloader:  3900\n",
      "length of test data: 20800\n",
      "length of test_dataloader:  650\n"
     ]
    }
   ],
   "source": [
    "# checkout what we created \n",
    "print(f\"Dataloader {train_dataloader , test_dataloader}\")\n",
    "print(\"length of train data:\",len(train_data))\n",
    "print(\"length of train_dataloader: \",len(train_dataloader))\n",
    "print(\"length of test data:\",len(test_data))\n",
    "print(\"length of test_dataloader: \",len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d6c03ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch , train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape , train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e0cc918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:torch.Size([1, 28, 28])\n",
      "label:1,label size:torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKeElEQVR4nO3dT4iW5R7H4d/raGGKMzAlYgVaQWi1KQqkiKBFFAO1KHKRuYoiCNq0i6BdlC6qhcugaBEEQQhB2LKMAltYLUPBkv6PIc1UM/OexYEvpzZnfk9H3znNdS1lvjw3NuPHR5270Xg8HhcAVNWGSR8AgLVDFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFFi3Tp8+XU8++WRdf/31tXnz5pqdna2HHnqoTp06NemjwcRsnPQBYFI+/fTT+uijj2r//v111VVX1alTp+rIkSN111131ZdfflmXXXbZpI8IF93I/0+B9WphYaE2b978px/7+OOPa9++ffX666/XgQMHJnQymBx/fMS69Z9B+OOPP+rHH3+s6667rmZmZurEiRMTPBlMjiiwbi0sLNRzzz1XV199dV166aV1+eWX1xVXXFHz8/N17ty5SR8PJsLfKbBuPfXUU/Xaa6/V008/Xfv27avp6ekajUa1f//+WllZmfTxYCJEgXXr7bffroMHD9bhw4fzY4uLizU/Pz+5Q8GE+eMj1q2pqan667+zePXVV2t5eXlCJ4LJ86bAujU3N1dvvPFGTU9P1969e+v48eN17Nixmp2dnfTRYGJEgXXr5ZdfrqmpqXrzzTdrcXGxbr/99jp27Fjdc889kz4aTIzvUwAg/J0CACEKAIQoABCiAECIAgAhCgDEqr9PYTQaXchzAHCBreY7ELwpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEBsnfQAmbzQatTcbNvR/PzEej9ubqqqVlZVBO6DPmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuBDvIhhyedzs7OygZ01PT7c31157bXuzY8eO9ub8+fPtTVXVZ5991t6cPn26vVleXm5v4J/GmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBArOsL8bZs2dLe3H333e3Nww8/3N7s2rWrvamqOnPmTHvzxRdftDdTU1PtzdzcXHtTVbVz58725v33329vnn/++fbmq6++am9gLfOmAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCj8Xg8XtUHjkYX+ix/6zm7d+9ub5544on25uabb25v3nvvvfbm6NGj7U1V1U8//dTe/Pzzz+3NkP9OQ37uqqpeeuml9ubWW29tb5599tn25vDhw+0NTMpqfrn3pgBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAbJz0Af7q/vvvH7R78cUX25sht28eOnSovfnuu+/am3+iO+64Y9Dutttua282bdrU3mzbtq29gX8abwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAcUEvxNuwod+cAwcODHrWzMxMe/Puu++2Ny63G27r1q2DdkMut1taWmpvfv311/YG/mm8KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDEmrsQ76abbhr0rG3btrU3Qy5o+/bbb9sb/u2dd94ZtHvwwQfbmz179rQ3d955Z3tz6NCh9mZ5ebm9gYvFmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAXNAL8VZWVtqbkydPDnrWNddc094MuQDtm2++aW8WFhbam7VuamqqvZmfnx/0rE8++aS9ufHGG9ubs2fPtjfj8bi94f/Dxo0X9JfHP1laWrpoz/pvvCkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAxJq7EO+ZZ54Z9KzFxcX25pVXXmlvHnjggfbmxIkT7c3Ro0fbm6qq8+fPtzf33ntve7Nnz572ZsgldVVV586da2+GfD488sgj7c2HH37Y3hw/fry9qRr29TTE7t2725vt27e3N0MvnNu1a1d7MxqN2pu5ubn25vvvv29vqqruu+++9mZ5eXnQs/4bbwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAxGg8Ho9X9YEDbhkcYuhzhtzsePDgwfbmhhtuaG/27t3b3nz99dftTVXV77//3t4MucV1yObkyZPtTdWwG08fe+yx9ubRRx9tb7Zs2dLeDLn1dahVfnn/ydatW9ubSy65pL0Z+rU+MzPT3gy5UfTMmTPtzQcffNDeVFU9/vjj7c2QW3NX8/ngTQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAg1tyFeGvdhg39jg7ZXExLS0uTPsL/3JDP19nZ2fbmlltuaW927NjR3lRVbd++vb354Ycf2ptffvmlvfn888/bm507d7Y3VVUvvPBCe3PkyJH25q233mpvfvvtt/amatjldkO4EA+AFlEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAwoV48DcM+boYekHikGet8sv7b2+GXOi2adOm9qaq6sorr2xvzp49294MvdxuLXMhHgAtogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEC/EA1gkX4gHQIgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgCxcbUfOB6PL+Q5AFgDvCkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQ/wLI8IGRpAUUEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show a sample\n",
    "# torch.manual_seed(42)\n",
    "random_idx = torch.randint(0,len(train_features_batch),size=[1]).item()\n",
    "img , label = train_features_batch[random_idx] , train_labels_batch[random_idx]\n",
    "\n",
    "plt.imshow(img.squeeze(),cmap = \"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n",
    "print(f\"Image size:{img.shape}\")\n",
    "print(f\"label:{label},label size:{label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5440a4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists , skipping download ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# download helper functions from learn pytorch repo \n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\"helper_functions.py already exists , skipping download ...\")\n",
    "else:\n",
    "    print(\"downloading helper_functions.py\")\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")# when dealing with github we need to pass raw link \n",
    "    with open(\"helper_functions.py\",\"wb\") as f:\n",
    "        f.write(request.content)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9af26b",
   "metadata": {},
   "source": [
    "### Creating a function to time our experiments\n",
    "\n",
    "Machine learning is very experimental .\n",
    "\n",
    "Two of the main things you'll often want to track are:\n",
    "1. Models performance (loss and accuracy values etc)\n",
    "2. How fast it runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4288a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    '''Prints difference between start and end time.'''\n",
    "    total_time = end - start\n",
    "    print(f'train time on {device} : {total_time:.3f} seconds')\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf13f0",
   "metadata": {},
   "source": [
    "# Creating a training loop and training  a model on batches of data \n",
    "\n",
    "1. Loop through epochs .\n",
    "2. Loop through training batches , perform training steps, calculate the train loss *per batch*\n",
    "3. Loop through testing batches , perform testing steps , calculate the test loss *per batch*\n",
    "4. Print out what's happening .\n",
    "5. Time it all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd97373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model : torch.nn.Module,\n",
    "              data_loader : torch.utils.data.DataLoader,\n",
    "              loss_fn : torch.nn.Module,\n",
    "              accuracy_fn):\n",
    "    \n",
    "    '''Returns a dictionary containing the results of model predicting on data_loader.'''\n",
    "    loss, acc = 0,0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for x , y in tqdm(data_loader) :\n",
    "            # Make predictions\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            # Accumulate the loss and accumulate values per batch \n",
    "            loss += loss_fn(y_pred,y)\n",
    "            acc += accuracy_fn(y_true = y ,y_pred = y_pred.argmax(dim = 1))\n",
    "        \n",
    "        # scale loss and acc to find the average loss/acc per batch\n",
    "        \n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name:\":model.__class__.__name__ , # only works when model was created with a class \n",
    "           \"model_loss\":loss.item(),\n",
    "            \"model_acc\":acc\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44682307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functionizing training loop\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "              data_loader:torch.utils.data.DataLoader,\n",
    "              loss_fn:torch.nn.Module,\n",
    "              optimizer:torch.optim.Optimizer,\n",
    "              accuracy_fn\n",
    "              ):\n",
    "    '''performs  a training with model trying to learn on data_loader'''\n",
    "    train_loss , train_acc = 0,0\n",
    "    \n",
    "    for batch , (x,y) in enumerate(data_loader):\n",
    "        model.train()\n",
    "        \n",
    "        # 1. Forward pass \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        train_loss +=loss # cumulate the train loss for every batch\n",
    "        \n",
    "        # 3. Caluculate accuracy\n",
    "        train_acc += accuracy_fn(y_true = y , y_pred = y_pred.argmax(dim=1))\n",
    "        \n",
    "        # 3. optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Loss backward \n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "    \n",
    "        #print out whats happening \n",
    "        if batch % 400 == 0:\n",
    "            print(f'looked at {batch * len(x)}/{len(data_loader.dataset)} samples .')\n",
    "    \n",
    "    # Divide total train_loss by length of train dataloader \n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"train loss: {train_loss:.5f} | train accuracy : {train_acc:.2f}% \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b941f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              data_loader:torch.utils.data.DataLoader,\n",
    "              loss_fn:torch.nn.Module,\n",
    "              optimizer:torch.optim.Optimizer,\n",
    "              accuracy_fn\n",
    "              ):\n",
    "    '''performs a testing with model trying to learn on data_loader'''\n",
    "    test_loss , test_acc = 0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for x_test,y_test in test_dataloader :\n",
    "            # 1. forward pass\n",
    "            test_pred = model(x_test)\n",
    "            \n",
    "            # 2. test loss \n",
    "            test_loss += loss_fn(test_pred,y_test)\n",
    "            \n",
    "            # 3. Caluculate accuracy\n",
    "            test_acc += accuracy_fn(y_true = y_test , y_pred = test_pred.argmax(dim=1))       #argmax to get index pf which weight in labels is maximum \n",
    "            \n",
    "        # calculate the test loss average per batch\n",
    "        test_loss /= len(data_loader)\n",
    "        \n",
    "        # calculate the test Accuracy average per batch \n",
    "        test_acc /= len(data_loader)\n",
    "        \n",
    "        print(f\"Test Loss : {test_loss:.4f} , test acc: {test_acc:.4f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706501a",
   "metadata": {},
   "source": [
    "# Building a Convolutional Neural Network (CNN)\n",
    "\n",
    "CNN's are also known ConvoNets.\n",
    "CNN's are known for their capabilties to find patterns in visual data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e4747af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cnn explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bf57719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a concolutional neural network \n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__( self, input_shape:int,hidden_units:int,output_shape:int):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape ,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3 ,\n",
    "                      stride=1 ,\n",
    "                      padding = 1) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units ,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3 ,\n",
    "                      stride=1 ,\n",
    "                      padding = 1) ,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "            \n",
    "        ) # values we can set ourselves in our NN's are called hyperparameters \n",
    "        \n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units ,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3 ,\n",
    "                      stride=1 ,\n",
    "                      padding = 1) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units ,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3 ,\n",
    "                      stride=1 ,\n",
    "                      padding = 1) ,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7, #trick to calculate this\n",
    "                     out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv_block_1(x)\n",
    "#         print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "#         print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b000c118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(1, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1323, out_features=27, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = CNNModel(input_shape = 1,  #color units \n",
    "                   hidden_units = 27,\n",
    "                   output_shape = len(class_names)\n",
    "                  )\n",
    "\n",
    "model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358687ff",
   "metadata": {},
   "source": [
    "# Setup loss function , optimizer for cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1c0e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss function \n",
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params = model_2.parameters(),\n",
    "                           lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee6b46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a085b",
   "metadata": {},
   "source": [
    "## training and testing cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd970c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c235c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5f08276869448bbf35a4ea6894e956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looked at 0/124800 samples .\n",
      "looked at 12800/124800 samples .\n",
      "looked at 25600/124800 samples .\n",
      "looked at 38400/124800 samples .\n",
      "looked at 51200/124800 samples .\n",
      "looked at 64000/124800 samples .\n",
      "looked at 76800/124800 samples .\n",
      "looked at 89600/124800 samples .\n",
      "looked at 102400/124800 samples .\n",
      "looked at 115200/124800 samples .\n",
      "train loss: 0.52217 | train accuracy : 83.94% \n",
      "\n",
      "Test Loss : 0.2999 , test acc: 90.4135%\n",
      "\n",
      "looked at 0/124800 samples .\n",
      "looked at 12800/124800 samples .\n",
      "looked at 25600/124800 samples .\n",
      "looked at 38400/124800 samples .\n",
      "looked at 51200/124800 samples .\n",
      "looked at 64000/124800 samples .\n",
      "looked at 76800/124800 samples .\n"
     ]
    }
   ],
   "source": [
    "# measure time\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer \n",
    "train_time_start = timer()\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_step(model = model_2,\n",
    "              data_loader=train_dataloader,\n",
    "               loss_fn = loss_fn,\n",
    "               optimizer = optimizer,\n",
    "               accuracy_fn=accuracy_fn)\n",
    "    \n",
    "    test_step(model = model_2,\n",
    "              data_loader=test_dataloader,\n",
    "               loss_fn = loss_fn,\n",
    "               optimizer = optimizer,\n",
    "               accuracy_fn=accuracy_fn)\n",
    "    \n",
    "train_time_end = timer()\n",
    "total_trian_time = print_train_time(start = train_time_start , end = train_time_end,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c75a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Train time in minutes : {total_trian_time//60} min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95147c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model_2 results \n",
    "\n",
    "model_2_results = eval_model(\n",
    "    model = model_2,\n",
    "    data_loader = test_dataloader,\n",
    "    loss_fn = loss_fn,\n",
    "    accuracy_fn = accuracy_fn   \n",
    ")\n",
    "\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions( model:torch.nn.Module,\n",
    "                    data: list,\n",
    "                    device: torch.device = \"cpu\"):\n",
    "    pred_probs = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for sample in data :\n",
    "            # prepare the sample (add a batch dimension and pass to target device )\n",
    "            sample = torch.unsqueeze(sample,dim=0)\n",
    "            \n",
    "            # forward pass (model outputs raw logits)\n",
    "            pred_logit = model(sample)\n",
    "            \n",
    "            # get prediction probability (logit -> prediction probability)\n",
    "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
    "            \n",
    "            # get pred_prob off the GPU for further calculations\n",
    "            pred_probs.append(pred_prob)\n",
    "            \n",
    "        #stack the pred_probs to turn list into a tensor \n",
    "        return torch.stack(pred_probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407981ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = CNNModel(input_shape = 1,  #color units \n",
    "                   hidden_units = 27,\n",
    "                   output_shape = len(class_names)\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60400fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'models/CNN_model_letter_detection.pth'\n",
    "model_2.load_state_dict(torch.load(PATH))\n",
    "model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e742998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# random.seed(42)\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "for sample , label in random.sample(list(test_data), k=9):\n",
    "    test_samples.append(sample)\n",
    "    test_labels.append(label)\n",
    "    \n",
    "# # view the first sample shape \n",
    "# test_samples[0].shape\n",
    "\n",
    "# make predictions\n",
    "pred_probs = make_predictions(model = model_2,\n",
    "                             data = test_samples)\n",
    "\n",
    "# # view first two prediction probabilites \n",
    "# pred_probs[:2]\n",
    "\n",
    "# convert prediction probabilities to labels \n",
    "pred_classes = pred_probs.argmax(dim=1)\n",
    "\n",
    "\n",
    "# Plot predictiions \n",
    "plt.figure(figsize=(9,9))\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    # create subplot \n",
    "    plt.subplot(nrows,ncols,i+1)\n",
    "    \n",
    "    # plot the target image\n",
    "    plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
    "    \n",
    "    # Find the prediction (in text form)\n",
    "    pred_label = class_names[pred_classes[i]]\n",
    "    \n",
    "    # get the truth label (in text form )\n",
    "    truth_label = class_names[test_labels[i]]\n",
    "    \n",
    "    # create a title for the plot \n",
    "    title_text = f\"pred : {pred_label} | Truth: { truth_label}\"\n",
    "    \n",
    "    # check for equality between pred and truth  and change color of title text\n",
    "    if pred_label == truth_label :\n",
    "        plt.title(title_text, fontsize = 10, c=\"g\") # green text if prediction same as truth \n",
    "    else:\n",
    "        plt.title(title_text, fontsize = 10,c='r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa1a9e",
   "metadata": {},
   "source": [
    "# making a confusion matrix for further prediction evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31567f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm.auto\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. make predictions with trained model\n",
    "y_preds = []\n",
    "model_2.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for x,y in tqdm(test_dataloader , desc =\"Making predictions ...\"):\n",
    "        \n",
    "        # do the forward pass\n",
    "        y_logit = model_2(x)\n",
    "        \n",
    "        # turn predictions from logits -> prediction probabilties -> prediction labels\n",
    "        y_pred = torch.softmax(y_logit.squeeze(),dim=0).argmax(dim=1)\n",
    "        \n",
    "        #put prediction on cpu for evaluation\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "# concatenate list of predictions into a tensor\n",
    "\n",
    "y_pred_tensor = torch.cat(y_preds)\n",
    "y_pred_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bda45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics, mlxtend\n",
    "mlxtend.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "import numpy\n",
    "# 2. Setup confusion instance and compare predictions to targets\n",
    "confmat = ConfusionMatrix(num_classes = len(class_names),task='multiclass')\n",
    "confmat_tensor = confmat(preds = y_pred_tensor , target = test_data.targets)\n",
    "\n",
    "# 3. plot the confusion matrix \n",
    "fig , ax = plot_confusion_matrix(conf_mat = confmat_tensor.numpy(),   #matplotlib likes working with numpy \n",
    "                                class_names = class_names,\n",
    "                                figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e11752",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded77196",
   "metadata": {},
   "source": [
    "# Save and load best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# create model directory path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents = True ,\n",
    "                exist_ok = True)\n",
    "\n",
    "# create model save \n",
    "MODEL_NAME = \"CNN_model_letter_detection.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
    "\n",
    "MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj = model_2.state_dict(),f = MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53326365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def display(im_path):\n",
    "    dpi = 80\n",
    "    im_data = plt.imread(im_path)\n",
    "    if(len(im_data.shape)==2):\n",
    "        height , width = im_data.shape\n",
    "    else:\n",
    "        height , width , depth = im_data.shape\n",
    "    \n",
    "    # what size does the figure need to be in inches to fit the image?\n",
    "    figsize = width / float(dpi) , height / float(dpi)\n",
    "    \n",
    "    # create a figure of the right size with one axes that takes up the ull figure \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    \n",
    "    # Hide spines, ticks etc.\n",
    "    ax.axis('off')\n",
    "    \n",
    "    #display the image\n",
    "    ax.imshow(im_data , cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_url = \"image_for_practice_.jpg\"\n",
    "\n",
    "\n",
    "def create_bounding_boxes(image_url):\n",
    "    image = cv2.imread(image_url)\n",
    "    \n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "#     print(\"gray\")\n",
    "#     cv2.imwrite(\"bbox_gray.jpg\",gray)\n",
    "#     display(\"bbox_gray.jpg\")\n",
    "    blur = cv2.GaussianBlur(gray,(7,7),0)        #object,size of blurring,\n",
    "#     print(\"blur\")\n",
    "#     cv2.imwrite(\"bbox_blur.jpg\",blur)\n",
    "#     display(\"bbox_blur.jpg\")\n",
    "    thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]       # blurr object , range ,\n",
    "#     print(\"thresh\")\n",
    "#     cv2.imwrite(\"bbox_thresh.jpg\",thresh)\n",
    "#     display(\"bbox_thresh.jpg\")\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(3,13))\n",
    "    \n",
    "    dilate = cv2.dilate(thresh, kernel , iterations=1)\n",
    "#     print(\"dilate\")\n",
    "#     cv2.imwrite(\"bbox_dilate.jpg\",dilate)\n",
    "#     display(\"bbox_dilate.jpg\")\n",
    "#     cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts)==2 else cnts[1]\n",
    "    \n",
    "    cnts = sorted(cnts, key=lambda x:cv2.boundingRect(x)[0])\n",
    "    i=1\n",
    "    word_bounding_boxes = []\n",
    "    for c in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if h<30:\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),(36,255,12),1)\n",
    "            word_bounding_boxes.append((x,y,w,h))\n",
    "            \n",
    "            # Crop the region from the image\n",
    "            cropped_image = image[y:y+h, x:x+w]           \n",
    "            \n",
    "            print(f\"bbox{i}\")\n",
    "            \n",
    "            \n",
    "            # Save the cropped image\n",
    "            cv2.imwrite(f'cropped_image{i}.jpg', cropped_image)\n",
    "            display(f'cropped_image{i}.jpg')\n",
    "            \n",
    "            i+=1\n",
    "#             cv2.imwrite(\"bbox_image.jpg\",image)\n",
    "#             display(\"bbox_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce360acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bounding_boxes(image_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
